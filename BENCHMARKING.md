# **Benchmarking Overview**

A **benchmark** is a standardized test designed to measure, compare, and evaluate the performance of a system, process, or component. Benchmarks follow a controlled and repeatable methodology, allowing organizations to objectively assess capabilities such as speed, efficiency, stability, and scalability across different environments.

Benchmarking is widely used in computing, networking, engineering, manufacturing, and finance. Its core purpose is to establish **performance baselines**, identify bottlenecks, support capacity planning, and guide architectural or technological decisions.

## **Core Characteristics of a Benchmark**

A proper benchmark must follow strict principles to ensure results are meaningful, reliable, and comparable across systems and configurations:

### **Repetible**

* Must produce *consistent and reproducible* results under the same conditions.
* Ensures results are trustworthy and free from randomness.

### **Standardized**

* Must follow a *fixed and well-defined methodology*, including workload, environment, and measurement procedure.
* Guarantees fairness and prevents biases caused by inconsistent testing.

### **Objective**

* Measurements must be *neutral and unbiased*, without subjective interpretation or manual interference.

### **Comparable**

* Results must enable *direct comparison* between different systems, hardware, models, or configurations.
* Ensures an apples-to-apples evaluation.

### **Measurable**

* Outputs must be expressed using *clear, quantifiable metrics* (time, throughput, resource usage, etc.).
* Enables precise interpretation and clear performance analysis.



# **Supported Metrics for Inference Benchmarking**

Below are the main metrics commonly used to evaluate inference systems such as vLLM. Each includes a definition, why it matters, and the primary influencing factors.


## **Time to First Token (TTFT)**

### **Definition**

* The time between when the server receives the request and when the first output token is produced.

### **Why It Matters**

* Reflects the perceived responsiveness for the user.
* Captures overhead that cannot be parallelized (scheduling, prompt processing, KV-cache setup).
* Critical for interactive applications such as chatbots, copilots, and agents.

### **Main Influencers**

* Prompt length
* Scheduling delays
* GPU load
* CUDA graph setup
* Contention for KV-cache pages

## **End-to-End Latency**

### **Definition**

* The total time from request arrival until the last output token is generated and returned.

### **Why It Matters**

* Represents the complete workflow execution time.
* Crucial for synchronous pipelines, RAG workflows, and automation tasks.

### **Main Influencers**

* Token generation speed
* Output length
* Batching behavior
* Queueing delays
* Overall system utilization

## **Output Throughput (tokens/second)**

### **Definition**

* The rate at which output tokens are generated by the model.

```
output_throughput = total_output_tokens / total_time
```

### **Why It Matters**

* Core indicator of GPU efficiency.
* Directly affects cost-per-token and system scalability.
* Shows the effectiveness of batching and parallel decoding.

### **Main Influencers**

* Attention performance
* Memory bandwidth
* Batching effectiveness
* Model size

## **Input Throughput (tokens/second)**

### **Definition**

* The rate at which the server can process input (prompt) tokens.

### **Why It Matters**

* For long-context workloads (32kâ€“128k+), prompt processing dominates latency.
* Measures how efficiently the system handles KV-cache prefill and long-context attention.

### **Main Influencers**

* Prompt length
* Prefill (input) performance
* KV-cache allocation
* Request distribution and variability

## **Requests per Second (RPS)**

### **Definition**

* The number of complete requests the server can handle per second.

### **Why It Matters**

* Represents real service-level capacity under multi-user workloads.
* More meaningful for production scenarios than raw token throughput alone.

### **Main Influencers**

* Variability in request sizes
* Scheduling and batching algorithms
* GPU resource contention
* Concurrency configuration

## **Error Rates**

### **Definition**

* The percentage of requests that fail due to server, hardware, or model issues.

### **Common Failure Types**

* HTTP 429 (server overloaded)
* HTTP 503 (scheduler cannot accept more requests)
* Timeout errors
* CUDA Out-of-Memory
* KV-cache fragmentation or insufficient pages
* Streaming interruptions or decoder failures

### **Why It Matters**

* High throughput with high error rates indicates an unstable or under-provisioned system.
* Reliability is as important as raw speed.

## **Token Statistics**

### **Definition**

Descriptive metrics that summarize the token distribution in the benchmark.
Common statistics include:

* Average input tokens
* Average output tokens
* Percentiles (p50, p90, p99)
* Input/output token ratio
* Workload mix characteristics

### **Why It Matters**

Token profiles vary drastically by use-case:

* **RAG:** long prompts, short outputs
* **Chat:** short prompts, long outputs
* **Agents:** highly variable workloads

Understanding token distribution is essential for interpreting throughput, latency, and overall system performance correctly.

# **Why Benchmarking Models and Infrastructure Is Important**

In the context of **LLM inference**, benchmarking is fundamental because:

* Model performance varies greatly based on architecture, context length, quantization, and optimization.
* Hardware selection (A100, H100, B100, L40S, 5090, etc.) changes throughput, latency, and cost per token.
* Serving engines (vLLM, TGI, TensorRT-LLM, custom runtimes) implement different batching, scheduling, and KV-cache strategies.
* Workloads differ widely: chat, RAG, batch inference, agents, streaming applications, etc.
* Infrastructure decisions (GPU count, memory limits, scaling strategy, networking) strongly influence performance and cost.

A solid benchmark helps determine:

* The most cost-efficient hardware for a given model
* The maximum sustainable concurrency
* SLAs for latency and throughput
* The stability and error tolerance of the serving stack
* How different models behave under realistic workloads

**Benchmarking is the only reliable way to compare models and infrastructure, understand real performance, and optimize deployments for production.**
